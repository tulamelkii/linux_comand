________________________________________________________________________________
________________________________Centos__________________________________________

######################network############################
ip link set <interface> up  # up interface
iwlist scan # scan network wi-fi
ip a | grep inet
ip route                    # show route
ip route add                # add route for network
#######################Date&chrony#######################

chronyc sources                          # check chronyd
timedatectl set-timezone Europe/Moscow   # install time and zone without reboot

####################Package##########################
rpm -qa
dnf list installed
dnf repolist all                  # show all repository
sudo dnf clean all                # clear cash
sudo dnf makecache                # check cash
dnf config-manager --set-enabled   <repo>
dnf repolist enabled | grep <repo>

dnf --showduplicates list <pkg>    #show all pkg for repos
pip3 list  #list installed



##################################Pacemaker#########
systemctl enable --now pcsd           #enable pcsd
pcs  cluster destroy                  #destroy cluster
sudo pcs host auth <host1> <host2>    #auth host cluster
!!disabled firewall and selinux!!!
pcs cluster setup <name_cluster!!>  <host1> <host2>    #create cluster
!!!/var/lib/pcsd/known-hosts!!! usermod -aG wheel hacluster!!!
pcs cluster start --all               # start cluster
pcs cluster enable --all              # emable cluster
pcs cluster status
pcs status corosync
pcs config
----------------------------------corosync------------------------
corosync-cfgtool -s

sudo pcs property set stonith-enabled=true  # enable stonith (stop or reboot node)
sudo pcs property set no-quorum-policy=stop  # Stop cluster ()

#########################selinux###################
cat /etc/selinux/config #SELINUX=disabled
sestatus #status SELINUX
setenforce 0 # disable online

##########################ports####################
lsof -i :<port>
lsof -u # process open user
lsof +D # Directory

########################disc#######################
df -h                                                     #full size diski
du . -hcd 1 | sort -hr                                         #sort for max_size
du -s *|sort -nr|cut -f 2-|while read a;do du -hs $a;done #sort size for folder

####################rabbitmq#######################

 rabbitmqctl delete_user "<user>"                         #dell user
 rabbitmqctl add_user <user> <password>                   #add_user
 rabbitmqctl set_permissions <user> ".*" ".*" ".*"        #full perm
 rabbitmqctl set_policy <name_policy>
rabbitmqctl clear_policy ha-all <name_policy>
 rabbitmqctl list_policies <list_policy>
 rabbitmqctl join_cluster --<ram or disk>  rabbit@<name_master_node> #add cluster nodes
rabbitmqctl cluster_status                               #status innodb_log_file_size
rabbitmqctl forget_cluster_node rabbit@Only              #forget nodes


#######################mariadb#####################
show databases;     #show db
use <database>;     #use db
show tables;        #show tables;
SELECT user,host FROM mysql.user;


##################ceph#############################
ceph status                        #
ceph osd pool ls detail            #details pool information
ceph orch host add <name_host> <ip_host>
ceph orch host ls                  # list host
-----------------------------------
ceph osd getcrushmap -o <crusch.txt>           # 1-get crushmap
crushtool -d {crush.txt} -o {decomp_crush.txt} # 2-decompile crushmap
crushtool -c {decompiled-crushmap-filename} -o {compiled-crushmap-filename} #recompile
ceph osd setcrushmap -i {compiled-crushmap-filename}

ceph osd pool set volumes pgp_num <size placement >
ceph osd pool set volumes pg_num <size placement group>
ceph osd pool set volumes size <size_replic>
ceph osd pool set volumes min_size <size_replic>

ceph osd pool set <name_pool> crush_rule <name rule>
ceph orch apply mon --placement="9 helius1 helius2 helius3 helius4 node0 node1 node2 node3 node4 " #add monitors
----------------------------------
ceph pg stat                       # statistic pg
ceph osd stat

 ceph osd blacklist ls
ceph osd blacklist rm <client-address>
ceph osd blacklist clear

--------------
---------------------------------rbd -------------------
rbd lock list volumes/volume<id_volume>
rbd map volumes/volume-<id_vol> /dev/rbd0 (mkdir /mnt/rbd-volume, mount /dev/rbd0p1  /mnt/rbd-volume)
----------show
rbd ls volumes |grep volume-eb
rbd info volumes/volume-eb
ceph osd map volumes rbd_data.3d3b4f8e63a28e


----------------------------------
ceph config set mon mon_allow_pool_delete true # allow delete Pools
ceph osd pool delete <name_pool> <name_pool>  --yes-i-really-really-mean-it #delete Pools
----------------------------------
rados df                          #static_pool
rbd ls --pool volumes             # show wolumes
-----------------------------------
ceph pg dump                      # show all os_user_domain_name
-----------------------------------
ceph osd crush rule ls            #ls rules crushmap
ceph osd crush rule dump          #show rules
------------------------------------------------test_rules-----------------
crushtool -c <decompl.txt> -o crushmap-new.bin #create bin for test.
crushtool --test -i <Create.bin> --show-utilization  --num-rep=2 | grep ^rule #test rule
---------------------------------------------------------------------------
ceph osd metadata 35 | grep device_path  # show disc
##############virsh##################################
virsh list --all       #on virtualmashine

virsh console --domain <vm>
###########useful#####################################
last                   # login to console
dmidecode -t bios -q   #check bios
stat <file>            #show option for filename
source ~/.bashrc       #обновить оболочку
env
crontab -l             # list rules
ssh-keygen
systemctl list-unit-files '*virt*'   #show service
systemd-analyze blame
systemd-analyze critical-chain
sudo systemctl list-dependencies <daemon>
locale

-------------ipmi_tool-------------------------------------
ipmitool -U admin -P admin -H <IP> power on

__________________________________________________________________________
_________________________Debian__________________________________________________
sudo apt list --installed   #list installed Package
dpkg-query -l               #list dpkg
apt -f install # fix packages
--------------------------------------------------
file <file>            #filename
stat <file>            # filename
whoami                 #show how you address
who                    #show tty and users


################apt####################################
apt list --upgradable        # show apgrade Package
apt policy <package name>    #  show information for Package
apt search  <pakage name>

_____________________________Docker____________________________________________
docker images
docker image rm <id images> --force
docker ps
docker run
--------------------------------------------------------------------------------
                                 gitlab

git clone git@gitlab...
git branch <show branch>
git add . <or file>
git commit -m "helm_vault"
git push origin devops


________________________________________________________________________________
_____________________________Kubernetes_________________________________________
kubectl get pods -A
kubectl get pods --all-namespaces   #show all pods
kubectl get nodes  --show-labels    # show nodes and labels
kubectl describe nodes <name node>
kubectl delete --all pods --namespace=<name space>
kubectl delete --all deployments --namespacfe=

*************namespaces********************

kubectl get namespace (ns)       # show namespaces
kubectl create namespace  (ns)   # create namespaces
************secret*************************

kubectl secret list
kubectl get secret -n <namespace>
kubectl describe secret <secret>

*********roles**********************

kubectl describe rolebinding -n <namespace> # show role to namespace
kubectl describe clusterrolebinding         # show cluster role
kubectl get clusterrole edit -o yaml 
kubectl get clusterrole cluster-admin -o yaml

***********************************



kubectl label nodes <name node> <labels> or --overwrite <label>
kubectl label pod <namme pod> <labels>
kubectl taint nodes <name node> <labels=value>:NoSchedule (-) # add tenat

kubectl get componentstatuses                       ##show helth
***************events****************

kubectl events
kubectl get events -n prometheus   # show journal

*************Port-forvard************
kubectl port-forward <pod_grafana> -n <namespace> --address 0.0.0.0

*************useful***
kubectl api-resources  #show all resources
kubectl get componentstatuses
kubectl get events --sort-by=.metadata.creationTimestamp  #check logs
kubectl get events -n prometheus   # show journal

----helm------------------------------------------------------------------------
helm install
helm repo list               # show install repo
helm search repo | grep <package>
helm show values <repo/path> # show values
helm get values <name_helm> -n <namespace>  # get value for chart
helm get matifest <name>
helm status prometheus -n default
helm rollback <helm>

-------------------------------------------------------------------------------
--------------------------------vault------------------------------------------
vault auth list # authintification
vault kv get <path>                                 #secret get
vault kv put <kv/dev.....>                          #secret add
vault kv list <path>                                #secret list
vault policy list                                   #policy list
vault policy read <policy>                          #read policy
vault policy write <name_policy>  <file_policy>.hcl #write policy
-----
path "kv/data/dev" {
  capabilities = ["list","read"]
} 
-----
vault list auth/kubernetes/role  #list role
vault read auth/kubernetes/config

-------------------------------------------------------------------------------
---------------------------Openstack cinder------------------------------------
cinder reset-state --state available <id_disk>
cinder reset-state --reset-migration-status <id_disk>
cinder reset-state --attach-status detached <id_disk>

openstack server add volume <id_server> <id_disk> --device /dev/vda

-------------------------------------------openstack server --------------------
openstack server set --state active <server_id>
openstack server list --host helius3 --all-projects -c ID --format value | xargs openstack server delete --force

--------------------------------------------placement---------------------------
openstack resource provider list                         # show id hosts
openstack resource provider inventory list <id_provider>      # show ressource host
resource provider allocation show <id_instace>           #show allocation vm
**********create_alocation**************
openstack resource provider allocation set     --project-id c084abdb62ee476d93b254aad0ee05a6     --user-id 6cf2c6e3327d4041817fc337     --allocation rp=9c92594b-a6fa-4e1e-83cd-7e1ce291376e,VCPU=2     --allocation rp=9c92594b-a6fa-4e1e-83cd-7e1ce291376e,MEMORY_MB=4096     6e2512e7-14bb-48ca-b2c3-f0c692a4a0b
*******************************************

-----------------------------------port neutron---------------------------------
openstack port list --server <instance_id>
openstack port show <id_port>

openstack image list && openstack network list && openstack availability zone list && openstack flavor list
--------------Network&Subnetwork---------------
openstack network create network_10_30_0_0 --internal --provider-network-type vxlan --share
openstack subnet create --allocation-pool start=10.30.0.3,end=10.30.0.243 --dhcp --dns-nameserver 8.8.8.8 --gateway 10.30.0.1 --network network_10_30_0_0 --subnet-range 10.30.0.0/24 sub_network_10_30_0_0
openstack server list --host node1 -c ID  --format value | tail| xargs -I {} openstack server migrate --live-migration --host helius1 --os-compute-api-version 2.30  {}
openstack server list --status ERROR -c ID --format value | xargs -I {} openstack server set --state active {}
--------------------
fix
nohup kubectl port-forward prometheus-grafana-bd979ff9-4hg5w -n prometheus --address 0.0.0.0 3000:3000 > port-forward.log 2>&1 &
